{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscope\n",
    "Generating horoscope-like tweets with different implementations of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import twitter as twt\n",
    "import seaborn as sns\n",
    "import codecs as cdc\n",
    "import pickle\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import itertools\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import random as rnd\n",
    "from collections import Counter, deque\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.despine()\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquire corpus\n",
    "---\n",
    "Initialize Twitter client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get('TWITTER_API_KEY', '')\n",
    "API_SECRET = os.environ.get('TWITTER_API_SECRET', '')\n",
    "BEARER_TOKEN = twt.oauth2_dance(API_KEY, API_SECRET)\n",
    "t = twt.Twitter(auth=twt.OAuth2(bearer_token=BEARER_TOKEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accounts = np.array(['gor_aries', 'gor_taurus', 'gor_gemini', 'gor_cancer', 'gor_leo', 'gor_virgo', \\\n",
    "                     'gor_libra', 'gor_scorpio', 'gor_sagittarius', 'gor_aquarius', \\\n",
    "                     'gor_pisces', 'gor_oven', 'gor_telec', 'gor_telec', 'gor_bliznecy', 'gor_rak', \\\n",
    "                     'gor_lev', 'gor_deva', 'gor_vesy', 'gor_skorpion', 'gor_strelec', 'gor_vodoley', \\\n",
    "                     'gor_riby'])\n",
    "months = np.array([u'января', u'февраля', u'марта', u'апреля', u'мая', u'июня', u'июля', u'августа', u'сентрября', \\\n",
    "                   u'октября', u'ноября', u'декабря'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(accounts, filename='raw'):\n",
    "    print 'Downloading data'\n",
    "    raw = []\n",
    "\n",
    "    for account in accounts:\n",
    "        print '-> Processing account %s' % account\n",
    "        max_id = 0\n",
    "        tweets = []\n",
    "\n",
    "        if not os.path.isfile('./raw_%s' % account):\n",
    "            print '--> Receiving tweets'\n",
    "            part = t.statuses.user_timeline(screen_name=account, include_rts='false', exclude_replies='true')\n",
    "            for tweet in part:\n",
    "                tweets.append(tweet)\n",
    "            old_max_id = max_id\n",
    "            max_id = tweets[len(tweets)-1]['id']\n",
    "\n",
    "            while old_max_id != max_id:\n",
    "                part = t.statuses.user_timeline(screen_name=account, \n",
    "                                                include_rts='false', exclude_replies='true', max_id=max_id, timeout=1)\n",
    "                for tweet in part:\n",
    "                    tweets.append(tweet)\n",
    "                old_max_id = max_id\n",
    "                max_id = tweets[len(tweets)-1]['id']\n",
    "            print '--> Received %d tweets' % len(tweets)\n",
    "\n",
    "            with open('raw_%s' % account, 'w') as fout:\n",
    "                pickle.dump(tweets, fout)\n",
    "            print '--> Dumped tweets'\n",
    "        else:\n",
    "            with open('raw_%s' % account, 'r') as fin:\n",
    "                tweets = pickle.load(fin)\n",
    "            print '--> Loaded previously dumped %d tweets' % len(tweets)\n",
    "        \n",
    "        tweets = [tweet['text'] for tweet in tweets]\n",
    "        for tweet in tweets:\n",
    "            raw.append(tweet)\n",
    "        del tweets\n",
    "\n",
    "    with open(filename, 'w') as fout:\n",
    "        pickle.dump(raw, fout)\n",
    "    del raw\n",
    "    \n",
    "def pad_data(data, pad_to=None, pad_with='<p>'):\n",
    "    if pad_to is None:\n",
    "        pad_to = max([len(item) for item in data])\n",
    "    \n",
    "    padded = []\n",
    "    for item in data:\n",
    "        amount = pad_to - len(item)\n",
    "        if amount < 0:\n",
    "            padded.append(item[:amount])\n",
    "        else:\n",
    "            padded.append(item + [pad_with] * amount)\n",
    "            \n",
    "    return padded\n",
    "\n",
    "def array_equals(a, b):\n",
    "    if len(a) != len(b):\n",
    "        return False\n",
    "    else:\n",
    "        length = len(a)\n",
    "        for i in range(length):\n",
    "            if a[i] != b[i]:\n",
    "                return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def clean_up(tokenized):\n",
    "    print '-> Cleaning up data'\n",
    "    \n",
    "    cleaned = []\n",
    "    \n",
    "    cleaned.append(tokenized[0])\n",
    "    length = 1\n",
    "    \n",
    "    for i in range(1, len(tokenized)):\n",
    "        j = 0\n",
    "        \n",
    "        while j < length:\n",
    "            if array_equals(tokenized[i], cleaned[j]):\n",
    "                break\n",
    "            j += 1\n",
    "        \n",
    "        if j == length:\n",
    "            cleaned.append(tokenized[i])\n",
    "            length += 1\n",
    "                \n",
    "    return cleaned\n",
    "\n",
    "def load_data(filename='./raw', load=False):\n",
    "    print 'Reading data'\n",
    "    \n",
    "    tokenized = []\n",
    "    words = []\n",
    "    \n",
    "    if not load:\n",
    "        print '-> With preprocesing'\n",
    "        with open(filename, 'r') as fin:\n",
    "            tweets = pickle.load(fin)\n",
    "\n",
    "        morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "        for tweet in tweets:\n",
    "            result = [u'<$>']\n",
    "            tweet = nltk.word_tokenize(tweet.lower())\n",
    "            tweet = [w for w in tweet if w not in string.punctuation]\n",
    "            tweet = [w for w in tweet if re.compile(u'[а-я]*').match(w).end() == len(w)]\n",
    "            tweet = [w for w in tweet if w not in months]\n",
    "            tweet = [morph.parse(w)[0].normal_form for w in tweet]\n",
    "            tweet.append(u'<#>')\n",
    "            result.extend(tweet)\n",
    "            tokenized.append(result)\n",
    "            \n",
    "        print '--> Before cleanup %d tweets' % len(tokenized)\n",
    "        tokenized = clean_up(tokenized)\n",
    "        tokenized = pad_data(tokenized)\n",
    "        print '--> After cleanup %d tweets' % len(tokenized)\n",
    "        words = np.concatenate(tokenized)\n",
    "\n",
    "        with open('tokenized', 'w') as fout:\n",
    "            pickle.dump(tokenized, fout)\n",
    "            \n",
    "        with open('words', 'w') as fout:\n",
    "            pickle.dump(words, fout)\n",
    "    else:\n",
    "        print '-> Previously processed'\n",
    "        with open('tokenized', 'r') as fin:\n",
    "            tokenized = pickle.load(fin)\n",
    "            \n",
    "        with open('words', 'r') as fin:\n",
    "            words = pickle.load(fin)\n",
    "            \n",
    "        print '--> Found %d tweets, %d tokens' % (len(tokenized), len(words))\n",
    "    \n",
    "    return tokenized, words\n",
    "\n",
    "def build_dataset(dataset, words, vocabulary_size=None):\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    word_count = []\n",
    "    tokenized = []\n",
    "    data = []\n",
    "    \n",
    "    word_count.extend(Counter(words).most_common(vocabulary_size))\n",
    "    print 'Most common word is %s with %d times' % (word_count[0][0], word_count[0][1])\n",
    "    print 'Least common word is %s with %d times' % (word_count[len(word_count)-1][0],\n",
    "                                                     word_count[len(word_count)-1][1])\n",
    "    \n",
    "    for word, _ in word_count:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "    \n",
    "    for item in dataset:\n",
    "        tmp = []\n",
    "        for word in item:\n",
    "            index = word_to_index[word]\n",
    "            tmp.append(index)\n",
    "            data.append(index)\n",
    "        tokenized.append(tmp)\n",
    "    \n",
    "    index_to_word = dict(zip(word_to_index.values(), word_to_index.keys()))\n",
    "    data = [word_to_index[w] for w in words]\n",
    "    \n",
    "    return tokenized, data, word_count, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-> Previously processed\n",
      "--> Found 10641 tweets, 308589 tokens\n"
     ]
    }
   ],
   "source": [
    "# download_data(accounts)\n",
    "tweets, words = load_data(load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common word is <p> with 140492 times\n",
      "Least common word is выступить with 1 times\n",
      "Using vocabulary size 7945\n"
     ]
    }
   ],
   "source": [
    "tokenized, data, word_count, word_to_index, index_to_word = build_dataset(tweets, words)\n",
    "vocabulary_size = len(word_count)\n",
    "print 'Using vocabulary size %d' % vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._text[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    true = np.zeros_like(predictions, dtype=np.float)\n",
    "    for i in range(len(labels)):\n",
    "        true[i, labels[i]] = 1.0\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(true, -np.log(predictions))) / true.shape[0]\n",
    "\n",
    "def perplexity(predictions, labels):\n",
    "    return np.exp(logprob(predictions, labels))\n",
    "\n",
    "def sample(distribution):\n",
    "    r = rnd.uniform(0, 1)\n",
    "    s = 0\n",
    "    \n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    \n",
    "    return len(distribution) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_size = 1000\n",
    "trunc_by = 20\n",
    "batch_size = 64\n",
    "num_nodes = 64\n",
    "embedding_size = 256\n",
    "valid_dataset = data[:valid_size]\n",
    "train_dataset = data[valid_size:]\n",
    "\n",
    "train_batches = BatchGenerator(train_dataset, batch_size, trunc_by)\n",
    "valid_batches = BatchGenerator(valid_dataset, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:\n",
    "$$\\mathcal{L} = - \\sum\\limits_{k = 1}^N y_k \\log o_k$$\n",
    "\n",
    "Output at step $k$:\n",
    "$$o_k = softmax(H_k \\ast W_{(sm)}^k + b_{(sm)}^k) = \\frac{\\exp \\{H_k \\ast W_{(sm)}^k + b_{(sm)}^k\\}}{\\sum\\limits_{n=1}^N \\exp \\{H_n \\ast W_{(sm)}^n + b_{(sm)}^n\\}}$$\n",
    "\n",
    "Hidden state at step $k$:\n",
    "$$H_k = output\\_gate(w_k, H_{k-1}) \\cdot \\tanh (state_k \\ast W_{(h)} + b_{(h)})$$\n",
    "\n",
    "Cell state at step $k$:\n",
    "$$state_k = forget\\_gate \\cdot state_{k-1} + input\\_gate \\cdot \\tanh ([w_k, h_{k-1}] \\ast W_{(c)} + b_{(c)})$$\n",
    "\n",
    "Output gate function for step $k$:\n",
    "$$output\\_gate(w_k, H_{k-1}) = \\sigma([w_k, h_{k-1}] W_{(o)} + b_{(o)})$$\n",
    "\n",
    "Forget gate function for step $k$:\n",
    "$$forget\\_gate(w_k, H_{k-1}) = \\sigma([w_k, h_{k-1}] W_{(f)} + b_{(f)})$$\n",
    "\n",
    "Input gate function for step $k$:\n",
    "$$output\\_gate(w_k, H_{k-1}) = \\sigma([w_k, h_{k-1}] W_{(i)} + b_{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    # Input data\n",
    "    train_data = [tf.placeholder(tf.int32, shape=[batch_size]) for i in range(trunc_by+1)]\n",
    "    train_inputs = train_data[:-1]\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Variables\n",
    "    # Embedding\n",
    "    embeddings = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], -0.1, 0.1), name='embeddings')\n",
    "    # The big matrix\n",
    "    Tw = tf.Variable(tf.truncated_normal([embedding_size + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    Tb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Connections\n",
    "    saved_output = tf.Variable(tf.truncated_normal([batch_size, num_nodes], -0.1, 0.1),\n",
    "                              name='saved_output')\n",
    "    saved_state = tf.Variable(tf.truncated_normal([batch_size, num_nodes], -0.1, 0.1),\n",
    "                          name='saved_state')\n",
    "    # Softmax\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size]),\n",
    "                                  name='softmax_weights')\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]),\n",
    "                                 name='softmax_biases')\n",
    "    \n",
    "    # Cell\n",
    "    def lstm_cell(i, state, cell):\n",
    "        i = tf.concat([i, state], 1)\n",
    "        tmp = tf.matmul(i, Tw) + Tb\n",
    "\n",
    "        inputs_gate = tf.sigmoid(tmp[:, :num_nodes])\n",
    "        forget_gate = tf.sigmoid(tmp[:, num_nodes:2*num_nodes])\n",
    "        output_gate = tf.sigmoid(tmp[:, 2*num_nodes:3*num_nodes])\n",
    "        update_gate = tf.tanh(tmp[:, 3*num_nodes:])\n",
    "\n",
    "        cell = forget_gate * cell + inputs_gate * update_gate\n",
    "        state = output_gate * tf.tanh(cell)\n",
    "        return state, cell\n",
    "\n",
    "    # Model\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for w in train_inputs:\n",
    "        e = tf.nn.embedding_lookup(embeddings, w)\n",
    "        output, state = lstm_cell(e, output, state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        sparse_outputs = tf.nn.dropout(tf.concat(outputs, 0), keep_prob=0.5)\n",
    "        logits = tf.nn.xw_plus_b(sparse_outputs, softmax_weights, softmax_biases)\n",
    "        true = tf.one_hot(indices=tf.concat(train_labels, 0), depth=vocabulary_size)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(true, 0), logits=logits)\n",
    "                              + 0.01 * tf.nn.l2_loss(Tw) + 0.01 * tf.nn.l2_loss(Tb))\n",
    "    \n",
    "    # Optimizer\n",
    "    gs = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(1e-2, gs, 3000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradients\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=gs)\n",
    "\n",
    "    # Prediction\n",
    "    train_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.nn.dropout(tf.concat(outputs, 0), keep_prob=0.5),\n",
    "                                                     softmax_weights, softmax_biases))\n",
    "    \n",
    "    # Evaluation\n",
    "    sample_input = tf.placeholder(tf.int32, [1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]), name='saved_sample_output')\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]), name='saved_sample_state')\n",
    "    \n",
    "    sample_embedding = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    tf.add_to_collection('ops', reset_sample_state)\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, softmax_weights, softmax_biases))\n",
    "        tf.add_to_collection('ops', sample_prediction)\n",
    "        \n",
    "    # Saving\n",
    "    to_save = {'Tw': Tw, 'Tb': Tb, 'saved_state': saved_state, 'saved_output': saved_output,\n",
    "               'softmax_weights': softmax_weights, 'softmax_biases': softmax_biases,\n",
    "               'saved_sample_output': saved_sample_output, 'saved_sample_state': saved_sample_state,\n",
    "               'embeddings': embeddings}\n",
    "    \n",
    "    saver = tf.train.Saver(to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 25.22 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 5.638076e+06\n",
      "==================================================================================================\n",
      "изысканный утренний независимый провокация далёкий металл независимый погладить отодвинуть квартира разлюбить заморачиваться независимый энд твёрдый доводить отодвинуть предсказуемый отодвинуть непостоянный шок юг средство освещать отодвинуть очковать поговоритьпо норма морда усладить твёрдый странный отодвинуть плачевный фальшь ситуация отодвинуть ожиданность фальшь\n",
      "==================================================================================================\n",
      "Validation set perplexity: 2.363686e+05\n",
      "\n",
      "Average loss at step 500: 3.73 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 2.584881e+01\n",
      "Validation set perplexity: 2.400397e+01\n",
      "\n",
      "Average loss at step 1000: 3.07 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.663725e+01\n",
      "Validation set perplexity: 2.267223e+01\n",
      "\n",
      "Average loss at step 1500: 2.93 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.224499e+01\n",
      "Validation set perplexity: 2.241888e+01\n",
      "\n",
      "Average loss at step 2000: 2.83 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.163457e+01\n",
      "Validation set perplexity: 2.313146e+01\n",
      "\n",
      "Average loss at step 2500: 2.76 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.210368e+01\n",
      "==================================================================================================\n",
      "если уже не видеться жарко\n",
      "==================================================================================================\n",
      "Validation set perplexity: 2.327089e+01\n",
      "\n",
      "Average loss at step 3000: 2.70 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.150781e+01\n",
      "Validation set perplexity: 2.475624e+01\n",
      "\n",
      "Average loss at step 3500: 2.56 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.191708e+01\n",
      "Validation set perplexity: 2.162406e+01\n",
      "\n",
      "Average loss at step 4000: 2.52 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.027118e+01\n",
      "Validation set perplexity: 2.183598e+01\n",
      "\n",
      "Average loss at step 4500: 2.50 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.142378e+01\n",
      "Validation set perplexity: 2.195825e+01\n",
      "\n",
      "Average loss at step 5000: 2.49 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.055074e+01\n",
      "==================================================================================================\n",
      "пофига серьёзно свобода сегодня тот кто везунчик кастинг\n",
      "==================================================================================================\n",
      "Validation set perplexity: 2.213647e+01\n",
      "\n",
      "Average loss at step 5500: 2.48 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.255826e+01\n",
      "Validation set perplexity: 2.266001e+01\n",
      "\n",
      "Average loss at step 6000: 2.46 learning rate 1.000000e-04\n",
      "Minibatch perplexity: 1.140465e+01\n",
      "Validation set perplexity: 2.218680e+01\n",
      "\n",
      "Average loss at step 6500: 2.44 learning rate 1.000000e-04\n",
      "Minibatch perplexity: 1.098941e+01\n",
      "Validation set perplexity: 2.172608e+01\n",
      "\n",
      "Average loss at step 7000: 2.43 learning rate 1.000000e-04\n",
      "Minibatch perplexity: 1.150940e+01\n",
      "Validation set perplexity: 2.174091e+01\n",
      "\n",
      "Average loss at step 7500: 2.42 learning rate 1.000000e-04\n",
      "Minibatch perplexity: 9.877152e+00\n",
      "==================================================================================================\n",
      "не единый а сегодня куча сомневаться в страна в тот кто не собираться\n",
      "==================================================================================================\n",
      "Validation set perplexity: 2.181230e+01\n",
      "\n",
      "Average loss at step 8000: 2.42 learning rate 1.000000e-04\n",
      "Minibatch perplexity: 9.937250e+00\n",
      "Validation set perplexity: 2.174000e+01\n",
      "\n",
      "Average loss at step 8500: 2.42 learning rate 1.000000e-04\n",
      "Minibatch perplexity: 9.734988e+00\n",
      "Validation set perplexity: 2.173320e+01\n",
      "\n",
      "Average loss at step 9000: 2.41 learning rate 1.000000e-05\n",
      "Minibatch perplexity: 8.782633e+00\n",
      "Validation set perplexity: 2.172267e+01\n",
      "\n",
      "Average loss at step 9500: 2.41 learning rate 1.000000e-05\n",
      "Minibatch perplexity: 1.001503e+01\n",
      "Validation set perplexity: 2.172266e+01\n",
      "\n",
      "Average loss at step 10000: 2.41 learning rate 1.000000e-05\n",
      "Minibatch perplexity: 8.909090e+00\n",
      "==================================================================================================\n",
      "быть но высокий общий расти любой время нет\n",
      "==================================================================================================\n",
      "Validation set perplexity: 2.171958e+01\n",
      "\n",
      "Average loss at step 10500: 2.41 learning rate 1.000000e-05\n",
      "Minibatch perplexity: 1.020738e+01\n",
      "Validation set perplexity: 2.171812e+01\n",
      "\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "print_every = 500\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        labels = np.concatenate(batches[1:])\n",
    "        feed_dict = {}\n",
    "        for i in range(trunc_by + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "\n",
    "        opt, l, pred, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % print_every == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / print_every\n",
    "            losses.append(average_loss)\n",
    "            print 'Average loss at step %d: %.2f learning rate %e' % (step, average_loss, lr)\n",
    "            average_loss = 0\n",
    "            labels = np.concatenate(batches[1:])\n",
    "            print 'Minibatch perplexity: %e' % perplexity(pred, labels)\n",
    "                  \n",
    "            if (step % (print_every * 5)) == 0:\n",
    "                sym = word_to_index['<$>']\n",
    "                reset_sample_state.run()\n",
    "                iter_num = 0\n",
    "                tweet = []\n",
    "                while sym != word_to_index['<#>'] and iter_num < 40:\n",
    "                    prediction = sample_prediction.eval({sample_input: np.array([sym])})[0]\n",
    "                    prediction = sample(prediction)\n",
    "                    if prediction == word_to_index['<$>']:\n",
    "                        break\n",
    "                    tweet.append(index_to_word[prediction])\n",
    "                    sym = prediction\n",
    "                    iter_num += 1\n",
    "                print '=' * 98\n",
    "                print ' '.join(tweet[:-1])\n",
    "                print '=' * 98\n",
    "                \n",
    "                saver.save(session, './Model/model')\n",
    "            \n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print 'Validation set perplexity: %e\\n' % np.exp(valid_logprob / valid_size)\n",
    "            if len(losses) > 2 and abs(losses[-2] - losses[-1]) < 1e-3:\n",
    "                print 'Converged'\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10fccc9d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIgAAAJTCAYAAABjBS0VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fccc550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore model to make new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> а вообще потом принцип не но у иногда стимул двигаться далёкий и к вы хороший просто решить боль\n",
      "--> смотреть на возможность че друг шанс это не стараться ведь вы стать видный не на про завтра верность человек обычно слишком важный идеальный внезапный трудность\n",
      "--> претендовать в весь даже не самый страшный последний время тем что супер результат\n",
      "--> голова возможно опережать они социальный или вкус\n",
      "--> скорпион не просто злиться на то нахрен вы любить\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    loader = tf.train.import_meta_graph('./Model/model.meta')\n",
    "    loader.restore(session, './Model/model')\n",
    "    \n",
    "    ops = tf.get_collection('ops')\n",
    "    reset_sample_state = ops[0]\n",
    "    sample_prediction = ops[1]\n",
    "    \n",
    "    for _ in range(5):\n",
    "        sym = word_to_index['<$>']\n",
    "        reset_sample_state.run(session=session)\n",
    "        iter_num = 0\n",
    "        tweet = []\n",
    "        while sym != word_to_index['<#>'] and iter_num < 40:\n",
    "            prediction = sample_prediction.eval({sample_input: np.array([sym])}, session)[0]\n",
    "            prediction = sample(prediction)\n",
    "            if prediction == word_to_index['<$>']:\n",
    "                break\n",
    "            tweet.append(index_to_word[prediction])\n",
    "            sym = prediction\n",
    "            iter_num += 1\n",
    "        print '--> %s' % ' '.join(tweet[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search to make better predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    loader = tf.train.import_meta_graph('./Model/model.meta')\n",
    "    loader.restore(session, './Model/model')\n",
    "    \n",
    "    ops = tf.get_collection('ops')\n",
    "    reset_sample_state = ops[0]\n",
    "    sample_prediction = ops[1]\n",
    "    \n",
    "    for _ in range(5):\n",
    "        maximum_length = 40\n",
    "        current_length = 0\n",
    "        tweet = ['<$>']\n",
    "        \n",
    "        sym = word_to_index['<$>']\n",
    "        reset_sample_state.run(session=session)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            candidates = {}\n",
    "            for prev in tweet:\n",
    "                for token in index_to_word.keys():\n",
    "                    \n",
    "        \n",
    "        while sym != word_to_index['<#>'] and current_length < maximum_length:\n",
    "            current_length += 1\n",
    "            prediction = sample_prediction.eval({sample_input: np.array([sym])}, session)\n",
    "            logprob = np.log(prediction)\n",
    "            logprob /= ((5 + current_length) / 6) ** 0.6 # length penalty\n",
    "            \n",
    "            \n",
    "            tweet.append(index_to_word[prediction])\n",
    "            sym = prediction\n",
    "        \n",
    "        print '--> %s' % ' '.join(tweet[1:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
